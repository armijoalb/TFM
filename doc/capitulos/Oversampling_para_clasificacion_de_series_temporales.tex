\chapter{Oversampling para clasificación de series temporales}
En este capítulo se describirán diferentes algoritmos de oversampling que se utilizarán más adelante en el estudio. Con dichos algoritmos de oversampling se busca mejorar el rendimiento de los modelos que se probarán en el estudio. Los algoritmos que a continuación se van a describir son SMOTE, MWMOTE y ADASYN.

\section{SMOTE}
SMOTE (Synthetic Minority Oversampling Technique) \cite{chawla2002smote} es uno de los algoritmos de oversampling más utilizados a día de hoy. Este algortimo fue creado en 2002 y actualmente se considera como uno de los algoritmos en el estado del arte del oversampling.\newline

Para balancear los datos de las diferentes clases, SMOTE propone generar instancias de la clase minoritaria de forma sintética. Para ello, SMOTE utiliza las instancias de la clase minoritaria para generar nuevas instancias de dicha clase con características parecidas a las ya existentes.\newline

Para conseguir dicho objetivo SMOTE sigue el siguiente proceso, elige una instancia de la clase minoritaria cualquiera, tras esto se buscan sus $K$ vecinos más cercanos que sean también de la clase minoritaria; dicho parámetro $K$ se puede cambiar y afecta a como son las instancias generadas sintéticamente; si por ejemplo su valor fuera $K=1$, las instancias se generan a partir de dos instancias de la clase minoritaria, esto hace que las características de dichas instancias nuevas sean muy parecidas a las de las instancias que se han utilizado; en cambio si $K$ tiene un valor muy grande entonces las instancias generadas tienen menos parecido a las instancias con las que se han generado.\newline

Una vez se han obtenido el número de vecinos elegidos, la nueva instancia se genera como una combinación de las instancias que se utilizan, por ejemplo si $K=1$, la nueva instancia se generaría como la media de los valores de dichas instancias. El algoritmo \ref{algo:smote} muestra el pseudocodigo de este algoritmo.\newline

\begin{algorithm}[H]
	\caption{SMOTE(T,N,k)}
	\label{algo:smote}
	\begin{algorithmic}[0]
		\State \textbf{Entrada:} Número de elementos de la clase minoritaria $T$; Cantidad de instancias creadas con SMOTE $N\%$; Número de vecinos $k$
		\State \textbf{Salida:} $(N/100) * T$ instancias sintéticas de la clase minoritaria.
		\State \textit{N} = $N/100$
		\State \textit{numattrs} = número de atributos
		\State \textit{Muestra} = vector de elementos de la clase minoritaria inicial.
		\State \textit{instancias\_generadas} = guarda el número de instancias generadas, inicialmente su valor es 0.
		\State \textit{Sinteticas} = vector de elementos para guardar instancias generadas.
		\State \textit{k\_vecinos} = vector para almacenar los $k$ vecinos más cercanos.
		\For{$i \gets 1,T$}
			\State Calcular los $k$ vecinos más cercanos a $i$ y guardar el resultado en $k\_vecinos$
			\While{$N \neq 0$}
				\State Elegir un elemento de $k\_vecinos$ de forma aleatoria y llamarlo $nn$.
				\For{$attr \gets 1,numattrs$}
					\State $dif = Muestra[nn][attr] - Muestra[i][attr]$
					\State $gap =$ Número aleatorio entre 0 y 1.
					\State $Sinteticas[instancias\_generadas][attr] = Muestra[i][attr] + gap*dif$
				\EndFor
				\State $instancias\_generadas ++$
				\State $N = N-1$
			\EndWhile
		\EndFor
	\end{algorithmic}
\end{algorithm}

La figura \ref{fig:41} muestra un ejemplo de generación de instancias con SMOTE.\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=100mm]{imagenes/SMOTE_comparison.png}
	\caption{Ejemplo de generación de instancias con SMOTE.}
	\label{fig:41}
\end{figure}
\verticalspace

Uno de los principales problemas de SMOTE es que no es bueno utilizarlo cuando las clases del conjunto de datos están mezcladas, es decir, dentro de los datos de la clase mayoritaria puede haber un pequeño conjunto de la clase minoritaria (la cual podría considerarse ruido); si se utilizara SMOTE, el número de instancias de ese conjunto sería mayor y esto afectaría al rendimiento de un modelo. Por ello, antes de utilizar este método es bueno realizar otro tipo de preprocesamientos como limpieza de ruido, imputación de valores perdidos, etc...
\newpage
\section{MWMOTE}
MWMOTE (Mayority Weighted Minority Oversampling Technique) \cite{barua2012mwmote} se trata de una modificación del algoritmo SMOTE para evitar de instancias ruidosas.\newline

El objetivo de este algoritmo es mejorar la forma de seleccionar instancias y la forma de generar las instancias sintéticas. Para ello este algoritmo cuenta con tres fases.\newline

En la primera fase se identifican los ejemplos de la clase minoritaria, a este conjunto lo podemos llamar $S_{min}$; que son más difíciles de aprender, para ello se genera un conjunto con dichos ejemplos al que llamaremos $S_{imin}$. Para generar este conjunto el algoritmo elimina todas aquellas instancias de $S_{min}$ que no tenga ninguna instancia de $S_{min}$ en su vecindario; para calcular el vecindario se utiliza el mismo proceso que para SMOTE. Una vez eliminados, se calculan aquellas instancias que son fronterizas con un conjunto de datos de la clase mayoritaria, estas instancias son las que forman $S_{imin}$.\newline

En la segunda fase, cada uno de los ejemplos de $S_{imin}$ se le agrega un peso dependiendo de la importancia de dicho ejemplo dentro de el conjunto de datos de $S_{min}$. Para ello se calcula la cantidad de información que se obtiene de cada uno de los puntos, esta medida se calcula teniendo en cuenta como de cercano está cada instancia al conjunto de datos de la clase mayoritaria. Por último, una vez calculado el peso de cada instancia se le asocia una probabilidad a dicho peso.\newline

En la tercera fase, el algoritmo genera instancias sintéticas a partir de $S_{imin}$ usando dicho peso y los añade al conjunto de datos de la clase minoritaria. Para generar dichas instancias, se detectan diferentes clústers dentro de $S_{min}$ y se por cada instancia que se genera se elige un elemento del conjunto $S_{imin}$ $x$, se elige otra instancia $y$ de $S_{min}$ que se encuentre en el mismo clúster que $x$ y se genera una nueva instancia de la siguiente forma: $ s = x + \alpha \times (y-x)$. El pseudocódigo de este algoritmo se puede encontrar en el algoritmo \ref{algo:mwmote}.

\newpage

\begin{algorithm}[H]
	\caption{MWMOTE($S_{maj},S_{min},N$)}
	\label{algo:mwmote}
	\begin{algorithmic}[0]
		\State \textit{Entrada:} Conjunto de datos de la clase mayoritaria $S_{maj}$; Conjunto de datos de la clase minoritaria $S_{min}$; Número de instancias a generar $N$
		\State \textit{Salida:} Conjunto de datos de la clase minoritaria ampliado $_{omin}$
		\ForAll{$x_i \in S_{min} $}
			\State Calcular su conjunto de vecinos más cercano, $NN(x_i)$
		\EndFor
		\State Calcular $S_{minf}$ como el conjunto de ejemplos de la clase minoritaria que no tiene ningún vecino de la clase minoritaria en su conjunto de vecinos más cercanos. $S_{minf} = S_{min} - \{x_i \in S_{min} : NN(x_i) $ no contiene ninguna instancia de $S_{min}  \}$
		\ForAll{$x_i \in S_{minf} $}
			\State Calcular conjunto de elementos más cercanos de la clase mayoritaria, $N_{maj}(x_i)$
		\EndFor
		\State Calcular el conjunto de la clase mayoritaria que forma frontera, $S_{bmaj} = \bigcup_{x_i \in S_{minf} } N_{maj}(x_i)$
		\ForAll{$y_i \in S_{bmaj}$}
			\State Calcular el conjunto de elementos más cercanos de la clase minoritaria, $N_{min} (y_i)$
		\EndFor
		\State Calcular el conjunto informativo minoritario, $S_{imin} = \bigcup_{y_i \in S_{bmaj}} N_{min}(y_i)$
		\ForAll{$y_i \in S_{bmaj}$}
			\State Calcular la cantidad de información, $I_w(y_i,x_i)$
		\EndFor
		\ForAll{$x_i \in S_{imin}$}
			\State Calcular peso de selección, $S_w(x_i) = \sum{y_i \in S_{bmaj}} I_w(y_i,x_i)$
		\EndFor
		\ForAll{$x_i \in S_{imin}$}
			\State Convertir $S_w(x_i)$ en probabilidad, $S_p(x_i) = S_w(x_i)/\sum{z_i \in S_{imin}}$
		\EndFor
		\State Calcular $M$ clúster sobre $S_{min}$, $L_1, L_2, ..., L_M$
		\State Inicializar $_{omin} = S_{min}$
		\For{$j \gets 1, N$}
			\State Seleccionar aleatoriamente un elemento $x$ de $S_imin$ con probabilidad $S_p(x)$; $x$ es un elemento perteneciente a un clúster $k$ de los calculados anteriormente.
			\State Seleccionar otra instancia $y$ aleatoriamente del clúster $k$.
			\State Crear una nueva instancia sintética $s$ como combinación lineal de las anteriores, $s = x + \alpha \times (y-x)$, donde $\alpha$ es un número aleatorio entre 0 y 1.
			\State Añadir $s$ a $S_{omin}$
		\EndFor
	\end{algorithmic}
\end{algorithm}

La figura \ref{fig:42} muestra un ejemplo de generación de instancias con MWMOTE.\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=100mm]{imagenes/MWMOTE_comparison.png}
	\caption{Ejemplo de generación de instancias con MWMOTE.}
	\label{fig:42}
\end{figure}
\verticalspace

\section{ADASYN}
ADASYN (Adaptive Synthetic Sampling) \cite{he2008adasyn} se trata de un algoritmo motivado por SMOTE. Al igual que el algoritmo de la sección anterior, propone otra forma de generar instancias. El objetivo principal de este algoritmo es reducir el sesgo entre ambas clases mientras aprende de forma adaptada. el procedimiento que sigue este algoritmo es el siguiente.\newline

Primero, calcula el ratio entre instancias de la clase minoritaria y mayoritaria; este ratio después se utiliza para calcular el número de instancias de la clase minoritaria a generar.\newline

Tras esto, por cada instancia perteneciente a la clase minoritaria, se calcula su valor $r_i$, este valor indica la dominancia de ejemplos de la clase mayoritaria en el vecindario de esta instancia; a valores mayores de esta medida es más difícil aprender sobre dicha instancia. Una vez calculadas todas las medidas se normalizan.\newline

Cuando todas las medidas $r_i$ han sido calculadas, se calcula el número de instancias a generar en el vecindario de dicha instancia de la siguiente manera: $G_i = G r_i$; donde $G$ es el número de instancias a generar que se calculó al principio.\newline
\newpage

Por último, cada una de las instancias en un vecindario $i$ se genera de la siguiente forma: se selecciona una instancia del vecindario, $x_i$; después se selecciona de forma aleatoria otra instancia vecina a la instancia $x_i$, la llamaremos $x_{zi}$; por último se calcula un número aleatorio entre 0 y 1 y se calcula la nueva instancia como la siguiente fórmula: $ s_i = x_i + (x_{zi}-x_i)\lambda$. El algoritmo \ref{algo:adasyn} muestra el pseudocodigo del algoritmo \textit{ADASYN}.

\begin{algorithm}
	\caption{ADASYN($D_{tr}, \beta, K$)}
	\label{algo:adasyn}
	\begin{algorithmic}[0]
		\State \textbf{Entrada:} Conjunto de datos de entrada $D_{tr}$; Parámetro para regular cantidad de ejemplos a crear $\beta$; Número de vecinos a tener en cuenta $K$.
		\State \textbf{Salida:} Conjunto de datos con oversampling de la clase minoritaria.
		\State $m_s$ = Número de instancias de la clase minoritaria.
		\State $m_j$ = Número de instancias de la clase mayoritaria.
		\State $d_{th}$ = Umbral máximo de desbalanceo
		\State Calcular grado de desbalanceo de clases, $d = m_s/m_j$
		\If{ $d < d_{th} $}
			\State Calcular número de instancias a generar $G = (m_j - m_s) \times \beta$
			\ForAll{$x_i \in m_s$}
				\State Calcular los $K$ vecinos más cercanos de $x_i$
				\State Calcular la dominancia de la clase positiva de $x_i$, $r_i = \bigtriangleup_i / K$, donde $\bigtriangleup_i$ es el número de vecinos de la clase positiva dentro de los $K$ vecinos de $x_i$
			\EndFor
			\State Normalizar todos los $r_i$ entre [0,1], para ello dividir cada uno sobre $\sum_{i=1}^{m_s} r_i$
			\State Calcular el número de instancias a generar por cada instancia de la clase minoritaria, $g_i = r_i \times G$
			\ForAll{$x_i \in m_s$}
				\For{$j \gets 1,g_i$}
					\State Elegir aleatoriamente un ejemplo de la clase minoritaria vecino de $x_i$, $x_{zi}$
					\State Generar una instancia sintética $s_i = x_i + (x_i - x_{zi}) \times \lambda$ donde $\lambda$ es un número aleatorio entre [0,1].
					\State Añadir $s_i$ a conjunto de datos de entrenamiento $D_{tr}$
				\EndFor
			\EndFor
		\EndIf
	\end{algorithmic}
\end{algorithm}



La figura \ref{fig:43} muestra un ejemplo de generación de instancias con ADASYN.\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=100mm]{imagenes/ADASYN_comparison.png}
	\caption{Ejemplo de generación de instancias con ADASYN.}
	\label{fig:43}
\end{figure}
\verticalspace

Uno de los problemas de ADASYN es que puede generar instancias en vecindarios donde el número de instancias de la clase minoritaria es muy reducida, produciendo ruido.
